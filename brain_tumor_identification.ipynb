{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Brain tumors present a critical challenge in the medical field due to their complexity and the vital importance of early and accurate diagnosis. Early detection of brain tumors is crucial for improving patient outcomes and survival rates. As studies have shown, \"Catching tumors early often allows for more treatment options. Some early tumors may have signs and symptoms that can be noticed, but this is not always the case\" (Cancer.org, n.d.). \"Enhancing the accuracy and efficiency of early-stage detection can significantly impact treatment planning and prognostic evaluation\" (Cancer.org, n.d.). The real-world problem this project aims to solve is the need for a reliable and efficient method to accurately classify different types of brain tumors from MRI scans. Given the subtle differences between various tumor types and normal brain tissue, manual classification by radiologists can be time-consuming and prone to error, especially in high-pressure environments. This project seeks to address these challenges by developing a tool that assists in the accurate and rapid classification of brain tumors, ultimately supporting better clinical decision-making and improving patient care.\n",
    "\n",
    "## Stakeholders\n",
    "The primary stakeholders for this project include:\n",
    "\n",
    "**Medical Professionals:** Radiologists, neurologists, and oncologists who rely on MRI images for diagnosing and treating brain conditions. This model can serve as a decision-support tool, helping them quickly identify and classify tumors, which can be especially useful in busy clinical settings or in areas with a shortage of specialists.\n",
    "\n",
    "**Healthcare Providers:** Hospitals and clinics can benefit from integrating this model into their diagnostic workflows, potentially reducing the time required for diagnosis and enabling more efficient resource allocation.\n",
    "\n",
    "**Researchers:** Medical researchers and data scientists working in the field of medical imaging and diagnostics can use the findings from this project to further refine and develop new models for tumor classification, potentially contributing to advances in AI-assisted diagnostics.\n",
    "\n",
    "**Patients:** Indirectly, patients stand to benefit from more accurate and timely diagnoses, which can lead to better treatment outcomes and reduced anxiety during the diagnostic process.\n",
    "\n",
    "The project’s outcomes could be used to improve diagnostic accuracy and speed, reduce the cognitive load on medical professionals, and ultimately lead to better health outcomes for patients. By providing a robust model for classifying brain tumors, this project could play a critical role in the early detection and treatment of brain cancer, which is crucial for improving survival rates and quality of life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "The dataset used in this project was obtained from Kaggle, specifically from the \"Brain Tumor Classification\" dataset. This dataset contains brain MRI images, categorized into four distinct classes: glioma tumors, meningioma tumors, pituitary tumors, and normal brain scans. The data was downloaded directly from the Kaggle platform and was provided in a well-organized structure, which facilitated the initial exploration and preprocessing stages.\n",
    "\n",
    "## Data Suitability\n",
    "The dataset is suitable for the objectives of this project, particularly for binary classification tasks. It provides a diverse set of images representing different types of brain tumors and normal cases, making it appropriate for training and evaluating convolutional neural networks (CNNs). However, the dataset was found to be inadequate for multiclass classification due to its limited size. The relatively small number of images in each class, particularly in the normal category, impacted the model's ability to accurately differentiate between the four classes in a multiclass setting. This limitation necessitated the use of techniques to address class imbalance and suggests that larger, more diverse datasets may be required for more effective multiclassification.\n",
    "\n",
    "## Dataset Description\n",
    "The dataset consists of MRI images divided into four categories:\n",
    "\n",
    "**Glioma Tumor:** 901 images\n",
    "\n",
    "**Meningioma Tumor:** 913 images\n",
    "\n",
    "**Pituitary Tumor:** 844 images\n",
    "\n",
    "**Normal:** 438 images\n",
    "\n",
    "Each image is labeled according to its category, and the images are of size 256x256 pixels. The dataset is organized into separate directories for each class, which were further split into training, validation, and test sets for this project.\n",
    "\n",
    "## Descriptive Statistics\n",
    "Descriptive statistics of the dataset reveal the distribution of images across the four classes. The tumor classes (glioma, meningioma, and pituitary tumors) have a relatively balanced number of images, while the normal class has fewer images, creating an imbalance. The dataset's image resolution is consistent at 256x256 pixels, providing uniformity in input size for the CNN models. The distribution across training, validation, and test sets was maintained proportionally during the data split.\n",
    "\n",
    "## Feature Justification\n",
    "The primary feature used in this project is the image data itself, specifically the pixel values of the MRI scans. The images serve as the input for the CNN models, which are designed to automatically learn relevant features during training. No additional manual feature extraction was performed, as CNNs are highly effective at identifying and learning important patterns directly from image data. The goal was to enable the models to learn the distinguishing features of different tumor types and normal brain scans.\n",
    "\n",
    "## Justification of Choice of Metrics \n",
    "In the context of the real-world problem of brain tumor classification, the selection of appropriate evaluation metrics is crucial for ensuring that the model meets the demands of medical applications. \n",
    "\n",
    "The primary metric of importance for this project is **recall**, particularly for the tumor classes. Recall is vital because it measures the model's ability to correctly identify all relevant instances—in this case, all actual tumor cases. In a medical context, a high recall is critical because failing to identify a tumor (a false negative) could have severe consequences, such as delayed treatment, progression of the disease, or even reduced survival rates. Therefore, ensuring that the model has a high recall helps to minimize the risk of missing a tumor diagnosis, which is a priority in clinical settings.\n",
    "\n",
    "**Precision** is the second most important metric. While recall focuses on minimizing false negatives, precision focuses on minimizing false positives, which is also important in a medical context. High precision ensures that when the model predicts a tumor, it is likely correct, thereby reducing the likelihood of unnecessary stress and potential harm to patients who might otherwise undergo further invasive testing or treatment based on incorrect predictions. Although precision is secondary to recall, it still plays a critical role in ensuring that the model's predictions are trustworthy and actionable.\n",
    "\n",
    "**Accuracy,** while a common metric, is less critical in this context due to the imbalanced nature of the dataset. Accuracy alone can be misleading, as it does not differentiate between the correct classification of tumor and nontumor cases. For instance, in a dataset where tumors are more prevalent, a model could achieve high accuracy simply by predicting the majority class (tumors) more often, without necessarily being effective at identifying the less common class (nontumors). Therefore, precision and recall are preferred metrics as they provide a more nuanced understanding of the model's performance, particularly in terms of its ability to handle the real-world consequences of errors in medical diagnosis.\n",
    "\n",
    "## Limitations of the Data\n",
    "The dataset has several limitations that impact the overall modeling process:\n",
    "\n",
    "**Class Imbalance:** The dataset is imbalanced, with significantly fewer normal images compared to tumor images. This imbalance poses challenges in training the model to accurately classify nontumor cases.\n",
    "\n",
    "**Limited Data Size:** While the dataset contains a reasonable number of images, the size may still be insufficient for achieving high accuracy in a complex task like brain tumor classification, particularly for multiclass classification.\n",
    "\n",
    "**Lack of Metadata:** The dataset does not include additional metadata (e.g., patient demographics, tumor size, or location), which could provide valuable context for the classification task.\n",
    "\n",
    "These limitations were considered during the modeling process, and steps were taken to mitigate their impact, such as using data augmentation and class weight adjustments to address class imbalance. However, the limitations suggest that further data collection or augmentation, as well as advanced modeling techniques, may be necessary for substantial improvements in classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial steps of this project, the raw image data, originally categorized into four distinct classes (glioma_tumor, meningioma_tumor, pituitary_tumor, and normal), was reorganized to facilitate binary classification. The goal of this reorganization was to simplify the classification task by focusing on distinguishing between tumor and nontumor images.\n",
    "\n",
    "To achieve this, the images from the three tumor-related classes (glioma_tumor, meningioma_tumor, and pituitary_tumor) were moved into a new directory named tumor. Simultaneously, the images from the normal class were relocated to a directory labeled nontumor. Once all the images were sorted into their respective new directories, the original four directories were deleted to streamline the dataset.\n",
    "\n",
    "After reorganizing the data, the next step involved splitting the images into training, validation, and test sets using an 80-10-10 ratio. This split was performed separately for both tumor and nontumor images to ensure that the class distribution remained proportional across the datasets. As a result, three main directories (train, val, and test) were created, each containing two subdirectories (tumor and nontumor), housing the images based on their designated split.\n",
    "\n",
    "Subsequently, data generators were set up using ImageDataGenerator from Keras. For the training data, data augmentation techniques such as rotation, width and height shifts, shear, zoom, and horizontal flips were applied to enhance the model's ability to generalize across different variations of the data. For the validation and test sets, only normalization was applied by rescaling the pixel values to a range of [0, 1].\n",
    "\n",
    "This preprocessing step was essential to prepare the dataset for binary classification, where the model's primary task was to differentiate between images showing tumors and those depicting normal brain scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing the data into tumor/nontumor and splitting it into train, val, and test sets\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.makedirs('tumor', exist_ok=True)\n",
    "os.makedirs('nontumor', exist_ok=True)\n",
    "\n",
    "for tumor_dir in ['glioma_tumor', 'meningioma_tumor', 'pituitary_tumor']:\n",
    "    for img in os.listdir(tumor_dir):\n",
    "        shutil.move(os.path.join(tumor_dir, img), 'tumor')\n",
    "\n",
    "for img in os.listdir('normal'):\n",
    "    shutil.move(os.path.join('normal', img), 'nontumor')\n",
    "\n",
    "shutil.rmtree('glioma_tumor')\n",
    "shutil.rmtree('meningioma_tumor')\n",
    "shutil.rmtree('pituitary_tumor')\n",
    "shutil.rmtree('normal')\n",
    "\n",
    "def split_data(source_dir, dest_dir, split_ratios=(0.8, 0.1, 0.1)):\n",
    "    files = os.listdir(source_dir)\n",
    "    train_files, temp_files = train_test_split(files, train_size=split_ratios[0], random_state=42)\n",
    "    val_files, test_files = train_test_split(temp_files, train_size=split_ratios[1]/(split_ratios[1] + split_ratios[2]), random_state=42)\n",
    "    \n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, 'train', os.path.basename(source_dir), file))\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, 'val', os.path.basename(source_dir), file))\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, 'test', os.path.basename(source_dir), file))\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(split, 'tumor'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(split, 'nontumor'), exist_ok=True)\n",
    "\n",
    "split_data('tumor', '.')\n",
    "split_data('nontumor', '.')\n",
    "\n",
    "shutil.rmtree('tumor')\n",
    "shutil.rmtree('nontumor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set - Tumor images: 2126, Nontumor images: 350\n",
      "Val set - Tumor images: 266, Nontumor images: 44\n",
      "Test set - Tumor images: 266, Nontumor images: 44\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of files in each directory and printing the number of images per category in each dataset\n",
    "import os\n",
    "\n",
    "def count_files(directory):\n",
    "    return sum([len(files) for r, d, files in os.walk(directory)])\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    tumor_count = count_files(os.path.join(split, 'tumor'))\n",
    "    nontumor_count = count_files(os.path.join(split, 'nontumor'))\n",
    "    print(f\"{split.capitalize()} set - Tumor images: {tumor_count}, Nontumor images: {nontumor_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2476 images belonging to 2 classes.\n",
      "Found 310 images belonging to 2 classes.\n",
      "Found 310 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data using ImageDataGenerator with augmentation for training and normalization for validation/test\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    'val',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    'test',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling (Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modeling process for binary classification involved a series of iterative improvements to identify the model that best balanced precision, recall, and overall accuracy. The journey began with a baseline CNN model, which served as the foundation. This baseline model performed well, achieving a validation accuracy of 94.52% with a precision of 0.8614 and recall of 0.8647. While these results were promising, there was still room for improvement, particularly in further enhancing recall, a key metric for this task.\n",
    "\n",
    "The next step was to develop a CNN model with added layers and dropout to increase complexity and reduce overfitting. This model improved the validation accuracy to 96.13%, but the recall slightly decreased to 0.8383, indicating a potential trade-off between accuracy and the model’s ability to correctly identify tumor cases. To address this, a more complex CNN model was designed, incorporating additional convolutional layers and batch normalization to stabilize learning. This model struck a better balance, achieving a validation accuracy of 93.87%, with precision at 0.8587 and recall at 0.8684. The improvement in recall, alongside balanced precision and accuracy, indicated that this model was better at identifying tumor cases while maintaining overall performance.\n",
    "\n",
    "Finally, an attempt was made to improve the baseline CNN by adjusting class weights to address the imbalance in the dataset. While this model did show strong results, it did not surpass the performance of the more complex CNN in terms of recall. As a result, the more complex CNN model was chosen as the final model due to its superior recall and balanced performance across all key metrics. This model’s ability to effectively identify the majority of tumor cases, while also maintaining strong precision and accuracy, made it the optimal choice for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Baseline Binary CNN model demonstrated solid performance with a validation loss of 0.1075 and a validation accuracy of 94.52%. The model’s precision was 0.8614, and its recall was 0.8647, indicating that it was effective at both predicting and correctly identifying tumor cases. The confusion matrix shows that the model correctly identified 230 out of 266 tumor cases, while it incorrectly identified 37 nontumor cases as tumors and missed 36 tumor cases.\n",
    "\n",
    "While the model performed well overall, its primary challenge lies in distinguishing nontumor cases from tumors, as reflected by the low true negative count of 7. This suggests that while the model is reliable in detecting tumors, there is room for improvement in reducing false positives, which could be achieved by further refining the model’s ability to differentiate between nontumor and tumor images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 180s 2s/step - loss: 0.4620 - accuracy: 0.8538 - val_loss: 0.4434 - val_accuracy: 0.8581\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 166s 2s/step - loss: 0.4164 - accuracy: 0.8586 - val_loss: 0.3669 - val_accuracy: 0.8581\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 163s 2s/step - loss: 0.3338 - accuracy: 0.8595 - val_loss: 0.2735 - val_accuracy: 0.8581\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 155s 2s/step - loss: 0.3145 - accuracy: 0.8574 - val_loss: 0.2881 - val_accuracy: 0.8581\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 155s 2s/step - loss: 0.2847 - accuracy: 0.8704 - val_loss: 0.2322 - val_accuracy: 0.8903\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.2527 - accuracy: 0.8724 - val_loss: 0.1949 - val_accuracy: 0.9097\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.2613 - accuracy: 0.8764 - val_loss: 0.1965 - val_accuracy: 0.9097\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.2265 - accuracy: 0.8938 - val_loss: 0.1683 - val_accuracy: 0.9290\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.2221 - accuracy: 0.8998 - val_loss: 0.3253 - val_accuracy: 0.8871\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.2052 - accuracy: 0.9059 - val_loss: 0.1843 - val_accuracy: 0.9226\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.2179 - accuracy: 0.9035 - val_loss: 0.2514 - val_accuracy: 0.9000\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 151s 2s/step - loss: 0.1870 - accuracy: 0.9192 - val_loss: 0.1444 - val_accuracy: 0.9355\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 152s 2s/step - loss: 0.1813 - accuracy: 0.9212 - val_loss: 0.1386 - val_accuracy: 0.9387\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 151s 2s/step - loss: 0.1944 - accuracy: 0.9148 - val_loss: 0.1860 - val_accuracy: 0.9194\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 151s 2s/step - loss: 0.1783 - accuracy: 0.9188 - val_loss: 0.2447 - val_accuracy: 0.9097\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 151s 2s/step - loss: 0.1653 - accuracy: 0.9253 - val_loss: 0.1075 - val_accuracy: 0.9452\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 152s 2s/step - loss: 0.1543 - accuracy: 0.9321 - val_loss: 0.2438 - val_accuracy: 0.9194\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 153s 2s/step - loss: 0.1513 - accuracy: 0.9301 - val_loss: 0.1193 - val_accuracy: 0.9419\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 152s 2s/step - loss: 0.1532 - accuracy: 0.9317 - val_loss: 0.1573 - val_accuracy: 0.9355\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 152s 2s/step - loss: 0.1487 - accuracy: 0.9362 - val_loss: 0.1253 - val_accuracy: 0.9452\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 153s 2s/step - loss: 0.1390 - accuracy: 0.9382 - val_loss: 0.1258 - val_accuracy: 0.9419\n",
      "10/10 [==============================] - 4s 417ms/step - loss: 0.1075 - accuracy: 0.9452\n",
      "Validation Loss: 0.10753267258405685\n",
      "Validation Accuracy: 0.9451612830162048\n",
      "10/10 [==============================] - 4s 402ms/step\n",
      "Confusion Matrix:\n",
      "[[  7  37]\n",
      " [ 36 230]]\n",
      "Precision: 0.8614232209737828\n",
      "Recall: 0.8646616541353384\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, evaluating, and saving a CNN model for binary classification\n",
    "\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import save_model\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "save_model(cnn_model, 'binary_cnn_model.h5')\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes)\n",
    "recall = recall_score(true_classes, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Added Layers and Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Binary CNN with added layers and dropout exhibited a validation loss of 0.0812 and an improved validation accuracy of 96.13%, reflecting a better fit on the validation data compared to the baseline model. The model's precision was 0.8447, and its recall was 0.8383, indicating that while it remained effective at predicting tumor cases, there was a slight drop in both precision and recall compared to the baseline model. The confusion matrix shows that the model correctly identified 223 out of 266 tumor cases, but it also produced 43 false negatives and 41 false positives.\n",
    "\n",
    "This model, despite its higher accuracy, had more difficulty in distinguishing between tumor and nontumor cases, as evidenced by the higher number of false negatives and false positives. The addition of layers and dropout helped prevent overfitting, but it also led to a decrease in the model's ability to accurately identify tumor cases, resulting in lower recall and precision. The increased complexity may have made the model less certain in its classifications, suggesting that further tuning of the architecture or additional regularization might be needed to strike a better balance between complexity and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.4637 - accuracy: 0.8489 - val_loss: 0.4793 - val_accuracy: 0.8581\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 351s 4s/step - loss: 0.4261 - accuracy: 0.8586 - val_loss: 0.3783 - val_accuracy: 0.8581\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 353s 5s/step - loss: 0.3817 - accuracy: 0.8586 - val_loss: 0.2737 - val_accuracy: 0.8581\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.3291 - accuracy: 0.8582 - val_loss: 0.2630 - val_accuracy: 0.8613\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.2740 - accuracy: 0.8748 - val_loss: 0.1977 - val_accuracy: 0.9161\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.2446 - accuracy: 0.8821 - val_loss: 0.2042 - val_accuracy: 0.9000\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 359s 5s/step - loss: 0.2402 - accuracy: 0.8885 - val_loss: 0.1391 - val_accuracy: 0.9419\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1987 - accuracy: 0.9095 - val_loss: 0.2492 - val_accuracy: 0.9032\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.2075 - accuracy: 0.9019 - val_loss: 0.1316 - val_accuracy: 0.9484\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1723 - accuracy: 0.9221 - val_loss: 0.3290 - val_accuracy: 0.8935\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1810 - accuracy: 0.9168 - val_loss: 0.1150 - val_accuracy: 0.9419\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 359s 5s/step - loss: 0.1567 - accuracy: 0.9285 - val_loss: 0.1302 - val_accuracy: 0.9419\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 360s 5s/step - loss: 0.1599 - accuracy: 0.9313 - val_loss: 0.1351 - val_accuracy: 0.9419\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1554 - accuracy: 0.9338 - val_loss: 0.1132 - val_accuracy: 0.9581\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1423 - accuracy: 0.9398 - val_loss: 0.1021 - val_accuracy: 0.9484\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1464 - accuracy: 0.9354 - val_loss: 0.1483 - val_accuracy: 0.9355\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1360 - accuracy: 0.9398 - val_loss: 0.1015 - val_accuracy: 0.9516\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1350 - accuracy: 0.9426 - val_loss: 0.1550 - val_accuracy: 0.9419\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.1231 - accuracy: 0.9451 - val_loss: 0.0991 - val_accuracy: 0.9516\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.1256 - accuracy: 0.9451 - val_loss: 0.1014 - val_accuracy: 0.9484\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1161 - accuracy: 0.9443 - val_loss: 0.1712 - val_accuracy: 0.9452\n",
      "Epoch 22/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.1244 - accuracy: 0.9426 - val_loss: 0.1314 - val_accuracy: 0.9355\n",
      "Epoch 23/50\n",
      "78/78 [==============================] - 359s 5s/step - loss: 0.1122 - accuracy: 0.9540 - val_loss: 0.0812 - val_accuracy: 0.9613\n",
      "Epoch 24/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.1174 - accuracy: 0.9487 - val_loss: 0.1045 - val_accuracy: 0.9484\n",
      "Epoch 25/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1036 - accuracy: 0.9564 - val_loss: 0.1109 - val_accuracy: 0.9452\n",
      "Epoch 26/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1106 - accuracy: 0.9527 - val_loss: 0.1100 - val_accuracy: 0.9516\n",
      "Epoch 27/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.1088 - accuracy: 0.9556 - val_loss: 0.3312 - val_accuracy: 0.9129\n",
      "Epoch 28/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1166 - accuracy: 0.9499 - val_loss: 0.0891 - val_accuracy: 0.9613\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.0812 - accuracy: 0.9613\n",
      "Validation Loss: 0.08121751993894577\n",
      "Validation Accuracy: 0.9612902998924255\n",
      "10/10 [==============================] - 11s 1s/step\n",
      "Confusion Matrix:\n",
      "[[  3  41]\n",
      " [ 43 223]]\n",
      "Precision: 0.8446969696969697\n",
      "Recall: 0.8383458646616542\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, evaluating, and saving an enhanced CNN model for binary classification\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "save_model(cnn_model, 'enhanced_cnn_model.h5')\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes)\n",
    "recall = recall_score(true_classes, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with More Added Layers and Adjusted Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Binary CNN with more added layers and an adjusted learning rate demonstrated a validation loss of 0.1410 and a validation accuracy of 93.87%. While the accuracy was slightly lower than that of the previous model, this model achieved the highest recall so far at 0.8684 and a comparably high precision of 0.8587. The confusion matrix indicates that the model correctly identified 231 out of 266 tumor cases, with 35 false negatives and 38 false positives.\n",
    "\n",
    "This model showed an improved balance between precision and recall, particularly in its ability to correctly identify tumor cases, which was the main objective. The higher recall signifies that this model was more effective at minimizing false negatives, making it particularly valuable given that missing a tumor could have serious consequences. However, the model still struggled with distinguishing nontumor cases, as seen in the 38 false positives. In summary, the adjustments in layers and learning rate helped the model achieve a well-rounded performance although it still struggles with identification of nontumor cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 446s 6s/step - loss: 0.4651 - accuracy: 0.8506 - val_loss: 0.4498 - val_accuracy: 0.8581\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 435s 6s/step - loss: 0.4333 - accuracy: 0.8586 - val_loss: 0.4363 - val_accuracy: 0.8581\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 360s 5s/step - loss: 0.3960 - accuracy: 0.8582 - val_loss: 0.3924 - val_accuracy: 0.8581\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 351s 5s/step - loss: 0.3482 - accuracy: 0.8599 - val_loss: 0.2986 - val_accuracy: 0.8581\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 351s 5s/step - loss: 0.3108 - accuracy: 0.8679 - val_loss: 0.2858 - val_accuracy: 0.8548\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.2855 - accuracy: 0.8679 - val_loss: 0.3042 - val_accuracy: 0.8548\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 351s 5s/step - loss: 0.2608 - accuracy: 0.8683 - val_loss: 0.1942 - val_accuracy: 0.9097\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.2505 - accuracy: 0.8833 - val_loss: 0.1831 - val_accuracy: 0.9194\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.2354 - accuracy: 0.8873 - val_loss: 0.1838 - val_accuracy: 0.9194\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 351s 5s/step - loss: 0.2332 - accuracy: 0.8926 - val_loss: 0.2737 - val_accuracy: 0.8871\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1902 - accuracy: 0.9107 - val_loss: 0.1410 - val_accuracy: 0.9387\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.2207 - accuracy: 0.9055 - val_loss: 0.3438 - val_accuracy: 0.8903\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.1887 - accuracy: 0.9152 - val_loss: 0.1468 - val_accuracy: 0.9323\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1805 - accuracy: 0.9156 - val_loss: 0.1662 - val_accuracy: 0.9194\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 353s 5s/step - loss: 0.2003 - accuracy: 0.9148 - val_loss: 0.3341 - val_accuracy: 0.8903\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1882 - accuracy: 0.9160 - val_loss: 0.1543 - val_accuracy: 0.9290\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.1410 - accuracy: 0.9387\n",
      "Validation Loss: 0.14099209010601044\n",
      "Validation Accuracy: 0.9387096762657166\n",
      "10/10 [==============================] - 11s 1s/step\n",
      "Confusion Matrix:\n",
      "[[  6  38]\n",
      " [ 35 231]]\n",
      "Precision: 0.8587360594795539\n",
      "Recall: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, evaluating, and saving a more complex CNN model with adjusted learning rate\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.00005),  \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "save_model(cnn_model, 'complex_cnn_model.h5')\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes)\n",
    "recall = recall_score(true_classes, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline CNN with Class Weight Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Binary CNN with class weight adjustment achieved a validation loss of 0.1149 and a validation accuracy of 94.84%. This model was designed to address class imbalance by assigning more weight to the minority class (nontumor) during training. The model’s precision was 0.8605, and its recall was 0.8346. The confusion matrix shows that the model correctly identified 222 out of 266 tumor cases, with 44 false negatives and 36 false positives.\n",
    "\n",
    "While the class weight adjustment helped maintain a high precision of 0.8605 for tumor cases, the recall slightly decreased to 0.8346 compared to other models. The model also showed a slight improvement in identifying nontumor cases, correctly classifying 8 out of 44 nontumor instances, though the performance in this area remained limited. The classification report further highlights the challenge, with a lower macro average F1-score of 0.51, reflecting the difficulty in balancing performance across both classes.\n",
    "\n",
    "Overall, the class weight adjustment was effective in maintaining precision but did not significantly enhance the model's ability to distinguish between tumor and nontumor cases. The relatively higher false negative rate suggests that while the model is generally reliable, it may still miss a notable number of tumor cases, indicating room for further improvement in handling class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 175s 2s/step - loss: 1.1742 - accuracy: 0.3045 - val_loss: 0.6908 - val_accuracy: 0.5290\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 1.0327 - accuracy: 0.6753 - val_loss: 0.3915 - val_accuracy: 0.8839\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 153s 2s/step - loss: 0.9133 - accuracy: 0.7888 - val_loss: 0.3710 - val_accuracy: 0.8871\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.7949 - accuracy: 0.8094 - val_loss: 0.2302 - val_accuracy: 0.8871\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 153s 2s/step - loss: 0.8329 - accuracy: 0.7888 - val_loss: 0.3856 - val_accuracy: 0.8645\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 153s 2s/step - loss: 0.6675 - accuracy: 0.8364 - val_loss: 0.2082 - val_accuracy: 0.9161\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 154s 2s/step - loss: 0.5559 - accuracy: 0.8586 - val_loss: 0.1653 - val_accuracy: 0.9258\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 153s 2s/step - loss: 0.6254 - accuracy: 0.8461 - val_loss: 0.1601 - val_accuracy: 0.9258\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 155s 2s/step - loss: 0.5256 - accuracy: 0.8760 - val_loss: 0.1901 - val_accuracy: 0.9323\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 154s 2s/step - loss: 0.4940 - accuracy: 0.8784 - val_loss: 0.2371 - val_accuracy: 0.9097\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 155s 2s/step - loss: 0.5177 - accuracy: 0.8667 - val_loss: 0.1429 - val_accuracy: 0.9323\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 156s 2s/step - loss: 0.3903 - accuracy: 0.8958 - val_loss: 0.1298 - val_accuracy: 0.9323\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 155s 2s/step - loss: 0.4351 - accuracy: 0.8918 - val_loss: 0.1363 - val_accuracy: 0.9419\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 155s 2s/step - loss: 0.3668 - accuracy: 0.9071 - val_loss: 0.1925 - val_accuracy: 0.9323\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 156s 2s/step - loss: 0.3589 - accuracy: 0.9128 - val_loss: 0.1296 - val_accuracy: 0.9516\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 156s 2s/step - loss: 0.3520 - accuracy: 0.9132 - val_loss: 0.1448 - val_accuracy: 0.9452\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 169s 2s/step - loss: 0.3462 - accuracy: 0.9075 - val_loss: 0.1149 - val_accuracy: 0.9484\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 173s 2s/step - loss: 0.3582 - accuracy: 0.9075 - val_loss: 0.1402 - val_accuracy: 0.9548\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 180s 2s/step - loss: 0.3256 - accuracy: 0.9148 - val_loss: 0.1238 - val_accuracy: 0.9548\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 168s 2s/step - loss: 0.2883 - accuracy: 0.9257 - val_loss: 0.1172 - val_accuracy: 0.9548\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 162s 2s/step - loss: 0.2897 - accuracy: 0.9265 - val_loss: 0.1646 - val_accuracy: 0.9484\n",
      "Epoch 22/50\n",
      "78/78 [==============================] - 178s 2s/step - loss: 0.3175 - accuracy: 0.9204 - val_loss: 0.1363 - val_accuracy: 0.9484\n",
      "10/10 [==============================] - 4s 420ms/step - loss: 0.1149 - accuracy: 0.9484\n",
      "Validation Loss: 0.11494480073451996\n",
      "Validation Accuracy: 0.948387086391449\n",
      "10/10 [==============================] - 5s 399ms/step\n",
      "Confusion Matrix:\n",
      "[[  8  36]\n",
      " [ 44 222]]\n",
      "Precision: 0.8604651162790697\n",
      "Recall: 0.8345864661654135\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.18      0.17        44\n",
      "           1       0.86      0.83      0.85       266\n",
      "\n",
      "    accuracy                           0.74       310\n",
      "   macro avg       0.51      0.51      0.51       310\n",
      "weighted avg       0.76      0.74      0.75       310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, and evaluating a CNN model with class weight adjustment \n",
    "\n",
    "class_weights = {0: 2126/350, 1: 1.0}  \n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "save_model(cnn_model, 'binary_cnn_with_class_weights.h5')\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes)\n",
    "recall = recall_score(true_classes, predicted_classes)\n",
    "class_report = classification_report(true_classes, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final evaluation of the binary classification model on the test data confirms the effectiveness of the chosen complex CNN model. The model achieved a test loss of 0.1526 and a test accuracy of 92.26%, indicating strong overall performance. However, the accuracy alone does not fully capture the model's performance, particularly in the context of an imbalanced dataset where correctly identifying tumor cases (class 1) is of primary importance.\n",
    "\n",
    "The confusion matrix shows that the model correctly identified 231 out of 266 tumor cases, yielding a recall of 0.8684. This high recall is crucial, as it means the model is highly effective at detecting the majority of tumor cases, minimizing the risk of false negatives, which is critical in medical applications. It is for this reason that recall was chosen as the primary metric for model selection. The precision for tumor cases was also 0.8684, indicating that when the model predicts a tumor, it is correct 86.84% of the time.\n",
    "\n",
    "However, the model continues to struggle with nontumor cases (class 0), correctly identifying only 9 out of 44 instances, resulting in a precision and recall of 0.20 for this class. This indicates that while the model is strong in identifying tumors, it has difficulty distinguishing nontumor images from tumors.\n",
    "\n",
    "Overall, the model's performance, with a weighted average F1-score of 0.77, reflects its balanced capability in identifying tumors. Despite its challenges with nontumor cases, the model's high recall and precision for tumor cases affirm its suitability for accurately detecting tumors. Further improvements could focus on enhancing the model’s ability to differentiate between tumor and nontumor images, potentially by collecting more diverse nontumor data or applying advanced techniques to address the imbalance.\n",
    "\n",
    "The implications of this final model evaluation are significant for solving the real-world problem of brain tumor detection. The high recall rate demonstrates that the model is highly effective in identifying the majority of tumor cases, which is critical for ensuring that patients receive timely and appropriate treatment which will result in better treatment outcomes. This is particularly important in medical settings, where the consequences of missing a tumor diagnosis can be severe. The model's ability to accurately detect tumors supports its potential use as a decision-support tool for radiologists and other medical professionals, helping to reduce the cognitive load and time required for manual classification of MRI scans. However, the challenges with correctly identifying nontumor cases could lead to unnecessary and costly treatments for healthy patients/ This suggests that the model should be used as an adjunct to, rather than a replacement for, human expertise, with further improvements necessary to enhance its reliability in distinguishing between different types of brain scans. This balance between automated detection and expert oversight could lead to better outcomes for patients by combining the strengths of both AI and human judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 11s 982ms/step - loss: 0.1526 - accuracy: 0.9226\n",
      "Test Loss: 0.152609184384346\n",
      "Test Accuracy: 0.9225806593894958\n",
      "10/10 [==============================] - 10s 959ms/step\n",
      "Confusion Matrix:\n",
      "[[  9  35]\n",
      " [ 35 231]]\n",
      "Precision: 0.868421052631579\n",
      "Recall: 0.868421052631579\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.20      0.20        44\n",
      "           1       0.87      0.87      0.87       266\n",
      "\n",
      "    accuracy                           0.77       310\n",
      "   macro avg       0.54      0.54      0.54       310\n",
      "weighted avg       0.77      0.77      0.77       310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the final model on the test data\n",
    "\n",
    "model = load_model('complex_cnn_model.h5')\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes)\n",
    "recall = recall_score(true_classes, predicted_classes)\n",
    "class_report = classification_report(true_classes, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Multiple Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for the multiple classification task, the image data originally split into binary classes (tumor vs. nontumor) was reorganized into four distinct categories: glioma_tumor, meningioma_tumor, pituitary_tumor, and normal. These categories were determined based on the first letter of the image filenames, with each image being moved into the corresponding directory.\n",
    "\n",
    "Once the data was organized into these four categories, a proportionate train-validation-test split was performed, maintaining an 80-10-10 ratio for each class. This ensured that each dataset had a balanced representation of the classes, which is crucial for training a robust classification model. After the split, ImageDataGenerators were configured for the training, validation, and test datasets. The training generator applied data augmentation techniques like rotation, shifts, and flips to enhance the model’s generalization capability, while the validation and test generators applied only rescaling.\n",
    "\n",
    "This preprocessing step set the stage for the development of multiple classification models aimed at distinguishing between the four different categories of brain images, laying the groundwork for training, validating, and testing these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glioma_tumor has 901 images\n",
      "meningioma_tumor has 913 images\n",
      "pituitary_tumor has 844 images\n",
      "normal has 438 images\n"
     ]
    }
   ],
   "source": [
    "# Reorganizing images into new directories for multiple classification\n",
    "\n",
    "\n",
    "os.makedirs('glioma_tumor', exist_ok=True)\n",
    "os.makedirs('meningioma_tumor', exist_ok=True)\n",
    "os.makedirs('pituitary_tumor', exist_ok=True)\n",
    "os.makedirs('normal', exist_ok=True)\n",
    "\n",
    "for dataset in ['train', 'val', 'test']:\n",
    "    for category in ['tumor', 'nontumor']:\n",
    "        category_path = os.path.join(dataset, category)\n",
    "        if os.path.exists(category_path):\n",
    "            for img in os.listdir(category_path):\n",
    "                if img.startswith('G'):\n",
    "                    shutil.move(os.path.join(category_path, img), 'glioma_tumor')\n",
    "                elif img.startswith('M'):\n",
    "                    shutil.move(os.path.join(category_path, img), 'meningioma_tumor')\n",
    "                elif img.startswith('P'):\n",
    "                    shutil.move(os.path.join(category_path, img), 'pituitary_tumor')\n",
    "                elif img.startswith('N'):\n",
    "                    shutil.move(os.path.join(category_path, img), 'normal')\n",
    "\n",
    "shutil.rmtree('train')\n",
    "shutil.rmtree('val')\n",
    "shutil.rmtree('test')\n",
    "\n",
    "for dir_name in ['glioma_tumor', 'meningioma_tumor', 'pituitary_tumor', 'normal']:\n",
    "    count = len(os.listdir(dir_name))\n",
    "    print(f\"{dir_name} has {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glioma_tumor - Train: 720, Val: 90, Test: 91\n",
      "meningioma_tumor - Train: 730, Val: 91, Test: 92\n",
      "pituitary_tumor - Train: 675, Val: 84, Test: 85\n",
      "normal - Train: 350, Val: 44, Test: 44\n"
     ]
    }
   ],
   "source": [
    "# Performing a train, val, test split \n",
    "\n",
    "def split_data(class_name, split_ratios=(0.8, 0.1, 0.1)):\n",
    "    files = os.listdir(class_name)\n",
    "    train_files, temp_files = train_test_split(files, train_size=split_ratios[0], random_state=42)\n",
    "    val_files, test_files = train_test_split(temp_files, train_size=split_ratios[1]/(split_ratios[1] + split_ratios[2]), random_state=42)\n",
    "\n",
    "    os.makedirs(f'train/{class_name}', exist_ok=True)\n",
    "    os.makedirs(f'val/{class_name}', exist_ok=True)\n",
    "    os.makedirs(f'test/{class_name}', exist_ok=True)\n",
    "\n",
    "    for file in train_files:\n",
    "        shutil.move(os.path.join(class_name, file), os.path.join(f'train/{class_name}', file))\n",
    "    for file in val_files:\n",
    "        shutil.move(os.path.join(class_name, file), os.path.join(f'val/{class_name}', file))\n",
    "    for file in test_files:\n",
    "        shutil.move(os.path.join(class_name, file), os.path.join(f'test/{class_name}', file))\n",
    "\n",
    "    return len(train_files), len(val_files), len(test_files)\n",
    "\n",
    "for class_name in ['glioma_tumor', 'meningioma_tumor', 'pituitary_tumor', 'normal']:\n",
    "    train_count, val_count, test_count = split_data(class_name)\n",
    "    print(f\"{class_name} - Train: {train_count}, Val: {val_count}, Test: {test_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2475 images belonging to 4 classes.\n",
      "Found 309 images belonging to 4 classes.\n",
      "Found 312 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setting up image generators for multiclass classification with augmentation for training and normalization for validation & testing\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    'val',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    'test',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling (Multiple Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modeling phase for multiple classification aimed to develop convolutional neural networks (CNNs) capable of classifying brain images into four categories: glioma_tumor, meningioma_tumor, pituitary_tumor, and normal. The initial baseline model served as a starting point, but the results indicated that the model struggled to accurately differentiate between the classes.\n",
    "\n",
    "Upon evaluating the baseline model, the metrics revealed significant challenges. The precision and recall scores were low across all classes, with accuracy at only 28%. The F1-scores, which balance precision and recall, were also low, indicating that the model was not effectively distinguishing between the classes. These results suggested that the model was not able to learn the distinct features necessary for accurate classification.\n",
    "\n",
    "To address these issues, subsequent models were developed with additional layers, more filters, and the inclusion of batch normalization to stabilize and improve learning. Despite these enhancements, the models continued to perform poorly, particularly in classifying the normal category, which had the smallest number of samples. The overall performance across all categories remained inadequate, likely due to the limited size of the dataset, which hindered the models' ability to generalize and learn the differences between the classes effectively.\n",
    "\n",
    "Ultimately, the final model, although slightly improved, still exhibited low performance metrics, indicating that the models were not capable of reliably distinguishing between the four classes. This highlighted the need for a larger and more balanced dataset to improve classification accuracy and the model's ability to generalize across different categories.\n",
    "\n",
    "In summary, the modeling process for multiple classification faced significant challenges due to the limited dataset size, resulting in poor performance metrics. The final model, while improved, was still unable to effectively distinguish between the four classes, underscoring the necessity for further data collection and possibly exploring alternative modeling approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Baseline CNN for multiple classification exhibited a validation loss of 0.5464 and a validation accuracy of 79.94%, which suggests that while the model was able to learn some features of the data, it struggled to generalize effectively across the four classes. The confusion matrix reveals that the model had difficulty correctly identifying the correct class for many samples, with significant overlap in its predictions across all tumor types and normal cases.\n",
    "\n",
    "The classification report reflects these challenges, with precision and recall scores across all classes being relatively low. The model achieved a precision of 0.24 for glioma tumors and 0.24 for normal cases, with slightly higher precision for meningioma tumors (0.30) and pituitary tumors (0.30). The recall was highest for meningioma tumors at 0.36, but for glioma tumors, it was only 0.17, indicating that the model struggled particularly with this class. The F1-scores across all classes hovered around 0.20-0.33, which underscores the model's difficulty in effectively distinguishing between the different types of tumors and normal cases.\n",
    "\n",
    "The baseline model’s primary weakness lies in its inability to accurately classify and distinguish between the four categories, as evidenced by the significant confusion across the classes. This model serves as a starting point, highlighting the need for further refinement and more complex architectures to improve the model’s ability to learn distinct features for each class and thereby enhance overall classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 157s 2s/step - loss: 1.3526 - accuracy: 0.3139 - val_loss: 1.3243 - val_accuracy: 0.3754\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 1.3132 - accuracy: 0.3640 - val_loss: 1.3036 - val_accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 151s 2s/step - loss: 1.1988 - accuracy: 0.4448 - val_loss: 1.1216 - val_accuracy: 0.5016\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 156s 2s/step - loss: 1.1557 - accuracy: 0.4760 - val_loss: 1.3208 - val_accuracy: 0.3333\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 165s 2s/step - loss: 1.0772 - accuracy: 0.5147 - val_loss: 0.8991 - val_accuracy: 0.6214\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 159s 2s/step - loss: 1.0283 - accuracy: 0.5410 - val_loss: 0.9856 - val_accuracy: 0.5340\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 148s 2s/step - loss: 1.0033 - accuracy: 0.5568 - val_loss: 0.8566 - val_accuracy: 0.6278\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 148s 2s/step - loss: 0.9515 - accuracy: 0.6044 - val_loss: 0.8601 - val_accuracy: 0.6181\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.9191 - accuracy: 0.6093 - val_loss: 1.0864 - val_accuracy: 0.5307\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.8845 - accuracy: 0.6149 - val_loss: 0.7231 - val_accuracy: 0.7346\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.8674 - accuracy: 0.6339 - val_loss: 0.7642 - val_accuracy: 0.6731\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.8131 - accuracy: 0.6646 - val_loss: 0.8039 - val_accuracy: 0.6731\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 147s 2s/step - loss: 0.7920 - accuracy: 0.6655 - val_loss: 0.8234 - val_accuracy: 0.6214\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 148s 2s/step - loss: 0.7614 - accuracy: 0.6780 - val_loss: 0.7227 - val_accuracy: 0.7087\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.7594 - accuracy: 0.6861 - val_loss: 0.7385 - val_accuracy: 0.6990\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.7452 - accuracy: 0.6804 - val_loss: 1.2586 - val_accuracy: 0.5599\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 147s 2s/step - loss: 0.7283 - accuracy: 0.6994 - val_loss: 0.7178 - val_accuracy: 0.7217\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.7179 - accuracy: 0.7006 - val_loss: 1.1245 - val_accuracy: 0.5987\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 152s 2s/step - loss: 0.6816 - accuracy: 0.7087 - val_loss: 0.8295 - val_accuracy: 0.6990\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.6764 - accuracy: 0.7285 - val_loss: 1.1729 - val_accuracy: 0.6117\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 148s 2s/step - loss: 0.6900 - accuracy: 0.7095 - val_loss: 1.1088 - val_accuracy: 0.6214\n",
      "Epoch 22/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.6584 - accuracy: 0.7293 - val_loss: 0.6726 - val_accuracy: 0.7314\n",
      "Epoch 23/50\n",
      "78/78 [==============================] - 148s 2s/step - loss: 0.6624 - accuracy: 0.7240 - val_loss: 0.5464 - val_accuracy: 0.7994\n",
      "Epoch 24/50\n",
      "78/78 [==============================] - 148s 2s/step - loss: 0.6246 - accuracy: 0.7560 - val_loss: 0.8478 - val_accuracy: 0.6699\n",
      "Epoch 25/50\n",
      "78/78 [==============================] - 148s 2s/step - loss: 0.5897 - accuracy: 0.7576 - val_loss: 0.6967 - val_accuracy: 0.7346\n",
      "Epoch 26/50\n",
      "78/78 [==============================] - 148s 2s/step - loss: 0.6097 - accuracy: 0.7519 - val_loss: 0.7677 - val_accuracy: 0.7087\n",
      "Epoch 27/50\n",
      "78/78 [==============================] - 148s 2s/step - loss: 0.5977 - accuracy: 0.7531 - val_loss: 0.6030 - val_accuracy: 0.7670\n",
      "Epoch 28/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.6035 - accuracy: 0.7600 - val_loss: 0.7006 - val_accuracy: 0.7217\n",
      "10/10 [==============================] - 4s 368ms/step - loss: 0.5464 - accuracy: 0.7994\n",
      "Validation Loss: 0.5464129447937012\n",
      "Validation Accuracy: 0.799352765083313\n",
      "10/10 [==============================] - 4s 379ms/step\n",
      "Confusion Matrix:\n",
      "[[15 31 15 29]\n",
      " [27 33 10 21]\n",
      " [ 5 15 11 13]\n",
      " [15 32 10 27]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor       0.24      0.17      0.20        90\n",
      "meningioma_tumor       0.30      0.36      0.33        91\n",
      "          normal       0.24      0.25      0.24        44\n",
      " pituitary_tumor       0.30      0.32      0.31        84\n",
      "\n",
      "        accuracy                           0.28       309\n",
      "       macro avg       0.27      0.28      0.27       309\n",
      "    weighted avg       0.27      0.28      0.27       309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, and evaluating a baseline CNN model for multiclass classification\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')  # 4 output units for the 4 classes\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',  # Loss function for multiclass classification\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "save_model(cnn_model, 'multiclass_cnn_model.h5')\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "class_report = classification_report(true_classes, predicted_classes, target_names=val_generator.class_indices.keys())\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model with Additional Layers, Filters, Dropout and a Reduced Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multiple Classification CNN model, enhanced with additional layers, filters, dropout, and a reduced learning rate, achieved a validation loss of 0.5051 and a validation accuracy of 81.23%. While the accuracy and loss indicate some improvement over the baseline model, the confusion matrix and classification report reveal that the model still struggled to effectively classify the various tumor types and normal cases.\n",
    "\n",
    "The confusion matrix shows significant confusion across all classes, with the model frequently misclassifying glioma tumors, meningioma tumors, pituitary tumors, and normal cases. For instance, out of 90 glioma tumor cases, only 28 were correctly classified, and the rest were spread across other categories. Similarly, normal cases were often misclassified, with only 4 out of 44 being correctly identified.\n",
    "\n",
    "The classification report highlights these challenges with low precision, recall, and F1-scores across all classes. The model achieved a precision of 0.31 for glioma tumors and 0.34 for meningioma tumors, but precision dropped significantly for normal cases (0.08). The recall was similarly low, particularly for normal cases, where it only reached 0.09. The overall accuracy of 28% and macro average F1-score of 0.25 underscore the model's continued difficulty in distinguishing between the different categories.\n",
    "\n",
    "Although this model introduced additional complexity and refined the learning process with a reduced learning rate, it still struggled to learn the distinct features necessary for accurate classification across all classes. The model’s performance suggests that further architectural adjustments, possibly combined with a larger and more balanced dataset, would be necessary to improve its ability to differentiate between the multiple classes effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 476s 6s/step - loss: 1.3685 - accuracy: 0.2820 - val_loss: 1.3623 - val_accuracy: 0.2945\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 469s 6s/step - loss: 1.3584 - accuracy: 0.3010 - val_loss: 1.3545 - val_accuracy: 0.4531\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 472s 6s/step - loss: 1.3346 - accuracy: 0.3463 - val_loss: 1.2584 - val_accuracy: 0.4045\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 471s 6s/step - loss: 1.1877 - accuracy: 0.4158 - val_loss: 1.2438 - val_accuracy: 0.4045\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 474s 6s/step - loss: 1.1480 - accuracy: 0.4420 - val_loss: 1.0501 - val_accuracy: 0.4563\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 433s 6s/step - loss: 1.1093 - accuracy: 0.4780 - val_loss: 0.9426 - val_accuracy: 0.6181\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 409s 5s/step - loss: 1.0758 - accuracy: 0.5026 - val_loss: 0.9520 - val_accuracy: 0.5437\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 422s 5s/step - loss: 1.0224 - accuracy: 0.5434 - val_loss: 0.8996 - val_accuracy: 0.5987\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 420s 5s/step - loss: 0.9970 - accuracy: 0.5531 - val_loss: 0.8925 - val_accuracy: 0.5987\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 414s 5s/step - loss: 0.9096 - accuracy: 0.6065 - val_loss: 0.9208 - val_accuracy: 0.6084\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 415s 5s/step - loss: 0.8997 - accuracy: 0.6198 - val_loss: 0.9025 - val_accuracy: 0.6343\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 413s 5s/step - loss: 0.8552 - accuracy: 0.6428 - val_loss: 1.0774 - val_accuracy: 0.5469\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 418s 5s/step - loss: 0.8289 - accuracy: 0.6578 - val_loss: 1.1109 - val_accuracy: 0.5761\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 418s 5s/step - loss: 0.8418 - accuracy: 0.6481 - val_loss: 0.6635 - val_accuracy: 0.7443\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 416s 5s/step - loss: 0.7772 - accuracy: 0.6667 - val_loss: 0.6459 - val_accuracy: 0.7087\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 449s 6s/step - loss: 0.7339 - accuracy: 0.6909 - val_loss: 0.6963 - val_accuracy: 0.7249\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 472s 6s/step - loss: 0.7131 - accuracy: 0.7103 - val_loss: 1.0108 - val_accuracy: 0.6375\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 470s 6s/step - loss: 0.7151 - accuracy: 0.6958 - val_loss: 0.5468 - val_accuracy: 0.7832\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 469s 6s/step - loss: 0.6758 - accuracy: 0.7273 - val_loss: 1.1494 - val_accuracy: 0.6278\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 470s 6s/step - loss: 0.6937 - accuracy: 0.7212 - val_loss: 0.7910 - val_accuracy: 0.7087\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 470s 6s/step - loss: 0.6599 - accuracy: 0.7349 - val_loss: 0.5051 - val_accuracy: 0.8123\n",
      "Epoch 22/50\n",
      "78/78 [==============================] - 445s 6s/step - loss: 0.6586 - accuracy: 0.7382 - val_loss: 0.7283 - val_accuracy: 0.7249\n",
      "Epoch 23/50\n",
      "78/78 [==============================] - 400s 5s/step - loss: 0.6353 - accuracy: 0.7503 - val_loss: 0.5695 - val_accuracy: 0.7508\n",
      "Epoch 24/50\n",
      "78/78 [==============================] - 403s 5s/step - loss: 0.6222 - accuracy: 0.7620 - val_loss: 0.7848 - val_accuracy: 0.7282\n",
      "Epoch 25/50\n",
      "78/78 [==============================] - 402s 5s/step - loss: 0.6089 - accuracy: 0.7612 - val_loss: 0.5496 - val_accuracy: 0.7929\n",
      "Epoch 26/50\n",
      "78/78 [==============================] - 402s 5s/step - loss: 0.5903 - accuracy: 0.7588 - val_loss: 0.6739 - val_accuracy: 0.7476\n",
      "10/10 [==============================] - 12s 1s/step - loss: 0.5051 - accuracy: 0.8123\n",
      "Validation Loss: 0.5050603747367859\n",
      "Validation Accuracy: 0.8122977614402771\n",
      "10/10 [==============================] - 12s 1s/step\n",
      "Confusion Matrix:\n",
      "[[28 29 13 20]\n",
      " [27 30 13 21]\n",
      " [14 10  4 16]\n",
      " [20 20 21 23]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor       0.31      0.31      0.31        90\n",
      "meningioma_tumor       0.34      0.33      0.33        91\n",
      "          normal       0.08      0.09      0.08        44\n",
      " pituitary_tumor       0.29      0.27      0.28        84\n",
      "\n",
      "        accuracy                           0.28       309\n",
      "       macro avg       0.25      0.25      0.25       309\n",
      "    weighted avg       0.28      0.28      0.28       309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, and evaluating an enhanced CNN model \n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4, activation='softmax')  \n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.00005),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "save_model(cnn_model, 'enhanced_multiclass_cnn_model.h5')\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "class_report = classification_report(true_classes, predicted_classes, target_names=val_generator.class_indices.keys())\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## CNN with More Layers, Filters, & Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model for multiple classification, enhanced with more layers, filters, and batch normalization, achieved a validation loss of 0.4836 and a validation accuracy of 83.17%. While these metrics suggest some improvement in the model's fit and stability, the confusion matrix and classification report indicate ongoing challenges in effectively distinguishing between the four classes.\n",
    "\n",
    "The confusion matrix shows that while the model improved slightly in identifying certain classes, significant confusion remains. For example, out of 90 glioma tumor cases, the model correctly identified 24, but it also misclassified many of these cases into other categories, particularly pituitary tumors. The normal cases were also frequently misclassified, with only 5 out of 44 being correctly identified.\n",
    "\n",
    "The classification report reflects these difficulties, with precision, recall, and F1-scores still relatively low across all classes. Glioma tumors had a precision of 0.34 and recall of 0.27, while meningioma tumors had even lower scores, with precision and recall around 0.27-0.26. The model particularly struggled with normal cases, achieving only 0.11 for both precision and recall. The overall accuracy was 26%, and the macro average F1-score was 0.24, indicating that the model's improvements did not significantly enhance its ability to differentiate between the different categories.\n",
    "\n",
    "Despite the additional layers, filters, and batch normalization, which were intended to improve the model’s generalization and stability, the model continued to struggle with classifying the various tumor types and normal cases. These results suggest that while architectural enhancements provided some benefits, further strategies—such as increasing the dataset size or using more advanced techniques—are necessary to improve the model's ability to accurately classify multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 700s 9s/step - loss: 2.1091 - accuracy: 0.4388 - val_loss: 2.6698 - val_accuracy: 0.2945\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 600s 8s/step - loss: 1.2651 - accuracy: 0.5568 - val_loss: 2.4654 - val_accuracy: 0.1424\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 588s 8s/step - loss: 0.9778 - accuracy: 0.6162 - val_loss: 1.2940 - val_accuracy: 0.3689\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 584s 7s/step - loss: 0.9296 - accuracy: 0.6222 - val_loss: 2.2204 - val_accuracy: 0.3528\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 579s 7s/step - loss: 0.8849 - accuracy: 0.6493 - val_loss: 1.2936 - val_accuracy: 0.5469\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 579s 7s/step - loss: 0.7994 - accuracy: 0.6683 - val_loss: 1.0702 - val_accuracy: 0.6343\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 581s 7s/step - loss: 0.7428 - accuracy: 0.6933 - val_loss: 1.0667 - val_accuracy: 0.6246\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 582s 7s/step - loss: 0.7354 - accuracy: 0.6962 - val_loss: 0.5742 - val_accuracy: 0.7929\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 578s 7s/step - loss: 0.6991 - accuracy: 0.7147 - val_loss: 0.9380 - val_accuracy: 0.6537\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 590s 8s/step - loss: 0.7007 - accuracy: 0.7184 - val_loss: 0.6546 - val_accuracy: 0.7508\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 582s 7s/step - loss: 0.6751 - accuracy: 0.7261 - val_loss: 0.7779 - val_accuracy: 0.6958\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 578s 7s/step - loss: 0.6529 - accuracy: 0.7422 - val_loss: 0.7582 - val_accuracy: 0.7282\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 585s 7s/step - loss: 0.6073 - accuracy: 0.7596 - val_loss: 0.4836 - val_accuracy: 0.8317\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 581s 7s/step - loss: 0.5955 - accuracy: 0.7511 - val_loss: 1.0786 - val_accuracy: 0.6440\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 579s 7s/step - loss: 0.6070 - accuracy: 0.7539 - val_loss: 0.5843 - val_accuracy: 0.7864\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 626s 8s/step - loss: 0.5783 - accuracy: 0.7620 - val_loss: 0.7197 - val_accuracy: 0.7249\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 646s 8s/step - loss: 0.5385 - accuracy: 0.7846 - val_loss: 0.7294 - val_accuracy: 0.7638\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 645s 8s/step - loss: 0.5204 - accuracy: 0.7903 - val_loss: 0.5435 - val_accuracy: 0.7832\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.4836 - accuracy: 0.8317\n",
      "Validation Loss: 0.48360633850097656\n",
      "Validation Accuracy: 0.8317152261734009\n",
      "10/10 [==============================] - 20s 2s/step\n",
      "Confusion Matrix:\n",
      "[[24 23  7 36]\n",
      " [22 24 19 26]\n",
      " [ 8 15  5 16]\n",
      " [17 26 13 28]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor       0.34      0.27      0.30        90\n",
      "meningioma_tumor       0.27      0.26      0.27        91\n",
      "          normal       0.11      0.11      0.11        44\n",
      " pituitary_tumor       0.26      0.33      0.29        84\n",
      "\n",
      "        accuracy                           0.26       309\n",
      "       macro avg       0.25      0.24      0.24       309\n",
      "    weighted avg       0.27      0.26      0.26       309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, and evaluating enhanced CNN model with more layers, increased filters, and batch normalization \n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4, activation='softmax')  \n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.00005),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "save_model(cnn_model, 'enhanced_multiclass_cnn_with_bn.h5')\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "class_report = classification_report(true_classes, predicted_classes, target_names=val_generator.class_indices.keys())\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Multiple Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of the final multiple classification model on the test data revealed significant challenges in accurately distinguishing between the four classes: glioma_tumor, meningioma_tumor, pituitary_tumor, and normal. While the model achieved a test accuracy of 85.58%, a closer examination of the confusion matrix and classification report highlights substantial issues. The confusion matrix showed that the model frequently misclassified glioma, meningioma, and pituitary tumors, often confusing them with one another. For instance, only 26 out of 91 glioma tumor samples were correctly identified, with many being misclassified as pituitary tumors. The model particularly struggled with the normal class, correctly identifying only 8 out of 44 samples, frequently mistaking normal brain images for various types of tumors.\n",
    "\n",
    "The classification report further underscored these difficulties, with precision and recall scores across all classes being low, particularly for the normal class, where precision and recall were both below 0.20. The overall accuracy, which was only 28%, along with the macro and weighted averages for precision, recall, and F1-scores hovering around 0.27-0.28, indicate that the model had significant difficulty in reliably distinguishing between the four categories. This poor performance is likely due to the limited size and imbalance of the dataset, which hindered the model's ability to learn distinct features for each class. The results suggest that further improvements, such as increasing the dataset size, enhancing data augmentation strategies, or exploring more advanced modeling techniques, are necessary to achieve better classification performance.\n",
    "\n",
    "The implications of the final multiple classification model's performance highlight the complexities and challenges of applying machine learning to the task of brain tumor classification across multiple types. The model's difficulties in accurately distinguishing between tumor types and normal brain tissue suggest that it is not yet reliable enough to be deployed in a clinical setting without significant further development. Misidentifying tumor cases could lead to delayed treatment resulting in worsened health outcomes. The model's misclassification of normal brain scans as tumors is also concerning, as it could lead to unnecessary stress and potential harm to patients through misdiagnosis and inappropriate treatment plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 20s 2s/step - loss: 0.4235 - accuracy: 0.8558\n",
      "Test Loss: 0.4234536588191986\n",
      "Test Accuracy: 0.8557692170143127\n",
      "10/10 [==============================] - 19s 2s/step\n",
      "Confusion Matrix:\n",
      "[[26 17 12 36]\n",
      " [27 26 15 24]\n",
      " [13 14  8  9]\n",
      " [19 28 11 27]]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor       0.31      0.29      0.30        91\n",
      "meningioma_tumor       0.31      0.28      0.29        92\n",
      "          normal       0.17      0.18      0.18        44\n",
      " pituitary_tumor       0.28      0.32      0.30        85\n",
      "\n",
      "        accuracy                           0.28       312\n",
      "       macro avg       0.27      0.27      0.27       312\n",
      "    weighted avg       0.28      0.28      0.28       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading, and evaluating my final multiple classification model on the test data\n",
    "\n",
    "cnn_model = load_model('enhanced_multiclass_cnn_with_bn.h5')\n",
    "\n",
    "test_loss, test_accuracy = cnn_model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "test_generator.reset()\n",
    "predictions = cnn_model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "class_report = classification_report(true_classes, predicted_classes, target_names=test_generator.class_indices.keys())\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Metrics\n",
    "The choice of metrics for evaluating the models in this project was guided by the critical need to minimize false negatives in the context of brain tumor detection. **Recall** was prioritized as the most important metric because it measures the model's ability to correctly identify all actual tumor cases. This is crucial in medical applications where missing a tumor could lead to delayed treatment and serious health consequences. **Precision** was the second most important metric, ensuring that when the model predicts a tumor, it is likely correct, thereby minimizing unnecessary stress and medical procedures for patients.\n",
    "\n",
    "## Final Model Selection\n",
    "Based on performance on the chosen metrics with the validation data, the Binary CNN with More Layers, Filters, & Adjusted Learning Rate was identified as the final model for deployment. This model achieved a balance between high recall and precision, particularly in detecting tumor cases. With a validation accuracy of 93.87% and a recall of 0.8684, it was deemed the most effective model for reliably identifying tumors while maintaining a reasonable level of precision.\n",
    "\n",
    "## Performance Evaluation on Holdout Test Data\n",
    "When evaluated on the holdout test data, the final binary model demonstrated a test loss of 0.1526 and a test accuracy of 92.26%. The model correctly identified 231 out of 266 tumor cases, yielding a recall of 0.8684, which is critical for ensuring that as many tumor cases as possible are detected. However, the model struggled with nontumor cases, correctly identifying only 9 out of 44 instances, resulting in a precision and recall of 0.20 for this class. While the binary model is suitable for deployment due to its high recall, none of the multiclass models are ready for deployment due to their significant challenges in accurately distinguishing between the four classes, particularly with poor performance in identifying normal brain scans.\n",
    "\n",
    "## Implications for Solving the Real-World Problem\n",
    "The final model's high recall rate has significant implications for solving the real-world problem of brain tumor detection. The ability to reliably identify tumor cases ensures that critical cases are not missed, which is vital in the early detection and treatment of brain tumors. This model can serve as an essential tool in clinical settings, where time is of the essence, and the stakes are high. By reducing the likelihood of missed diagnoses, the model could contribute to improved patient outcomes, particularly in cases where early intervention is possible and can lead to more effective treatment. However, the model’s difficulty in distinguishing nontumor cases from tumors also suggests that while it is a powerful aid, it should be used in conjunction with human expertise to ensure comprehensive diagnostic accuracy. This balance between automated detection and expert oversight is crucial in ensuring that the tool enhances, rather than replaces, the nuanced judgment required in medical diagnostics.\n",
    "\n",
    "## Limitations of the Data and Project\n",
    "Several limitations of the data and the project have impacted the model's performance. The **dataset's imbalance**—with significantly fewer nontumor images compared to tumor images—hindered the model's ability to accurately distinguish between the two classes. Additionally, the **limited size** of the dataset was a major constraint, particularly for multiclass classification, where the model struggled to learn distinct features for each tumor type. These limitations suggest that further data collection, augmentation, and exploration of more advanced modeling techniques are necessary to improve the model's performance.\n",
    "\n",
    "## Summary of Implications for the Real-World Problem and Stakeholders\n",
    "The implications of this project for the real-world problem of brain tumor detection are substantial. For **medical professionals**, the deployment of this model could lead to faster and more accurate diagnoses, particularly for identifying tumor cases that might otherwise be missed. This can alleviate the diagnostic burden on radiologists, especially in resource-constrained settings, and provide a safety net by ensuring that potential tumors are flagged for further review. **Healthcare providers** could benefit from integrating this model into diagnostic workflows, reducing the time required for initial assessments and potentially lowering costs associated with extensive testing. For **patients**, the model’s high recall means a higher likelihood of early detection, which is critical for successful treatment and improved survival rates. **Researchers** can also benefit from this project by building upon its findings to further refine models, potentially improving their accuracy and robustness in classifying brain tumors. However, the model's limitations underscore the importance of continuous development and the need for careful deployment, ensuring that it is used as an adjunct to, rather than a replacement for, clinical expertise. The project highlights the potential for AI in healthcare, but also the necessity of collaboration between technology and human judgment to achieve the best outcomes for patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **American Cancer Society. (n.d.). Brain and Spinal Cord Tumors in Adults - Detection, Diagnosis, and Staging. Retrieved from https://www.cancer.org/cancer/types/brain-spinal-cord-tumors-adults/detection-diagnosis-staging.html**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
