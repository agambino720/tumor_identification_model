{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing the data into tumor/nontumor and splitting it into train, val, and test sets\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.makedirs('tumor', exist_ok=True)\n",
    "os.makedirs('nontumor', exist_ok=True)\n",
    "\n",
    "for tumor_dir in ['glioma_tumor', 'meningioma_tumor', 'pituitary_tumor']:\n",
    "    for img in os.listdir(tumor_dir):\n",
    "        shutil.move(os.path.join(tumor_dir, img), 'tumor')\n",
    "\n",
    "for img in os.listdir('normal'):\n",
    "    shutil.move(os.path.join('normal', img), 'nontumor')\n",
    "\n",
    "shutil.rmtree('glioma_tumor')\n",
    "shutil.rmtree('meningioma_tumor')\n",
    "shutil.rmtree('pituitary_tumor')\n",
    "shutil.rmtree('normal')\n",
    "\n",
    "def split_data(source_dir, dest_dir, split_ratios=(0.8, 0.1, 0.1)):\n",
    "    files = os.listdir(source_dir)\n",
    "    train_files, temp_files = train_test_split(files, train_size=split_ratios[0], random_state=42)\n",
    "    val_files, test_files = train_test_split(temp_files, train_size=split_ratios[1]/(split_ratios[1] + split_ratios[2]), random_state=42)\n",
    "    \n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, 'train', os.path.basename(source_dir), file))\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, 'val', os.path.basename(source_dir), file))\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, 'test', os.path.basename(source_dir), file))\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(split, 'tumor'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(split, 'nontumor'), exist_ok=True)\n",
    "\n",
    "split_data('tumor', '.')\n",
    "split_data('nontumor', '.')\n",
    "\n",
    "shutil.rmtree('tumor')\n",
    "shutil.rmtree('nontumor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set - Tumor images: 2126, Nontumor images: 350\n",
      "Val set - Tumor images: 266, Nontumor images: 44\n",
      "Test set - Tumor images: 266, Nontumor images: 44\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of files in each directory and printing the number of images per category in each dataset\n",
    "import os\n",
    "\n",
    "def count_files(directory):\n",
    "    return sum([len(files) for r, d, files in os.walk(directory)])\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    tumor_count = count_files(os.path.join(split, 'tumor'))\n",
    "    nontumor_count = count_files(os.path.join(split, 'nontumor'))\n",
    "    print(f\"{split.capitalize()} set - Tumor images: {tumor_count}, Nontumor images: {nontumor_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2476 images belonging to 2 classes.\n",
      "Found 310 images belonging to 2 classes.\n",
      "Found 310 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data using ImageDataGenerator with augmentation for training and normalization for validation/test\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    'val',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    'test',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling (Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 180s 2s/step - loss: 0.4620 - accuracy: 0.8538 - val_loss: 0.4434 - val_accuracy: 0.8581\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 166s 2s/step - loss: 0.4164 - accuracy: 0.8586 - val_loss: 0.3669 - val_accuracy: 0.8581\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 163s 2s/step - loss: 0.3338 - accuracy: 0.8595 - val_loss: 0.2735 - val_accuracy: 0.8581\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 155s 2s/step - loss: 0.3145 - accuracy: 0.8574 - val_loss: 0.2881 - val_accuracy: 0.8581\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 155s 2s/step - loss: 0.2847 - accuracy: 0.8704 - val_loss: 0.2322 - val_accuracy: 0.8903\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.2527 - accuracy: 0.8724 - val_loss: 0.1949 - val_accuracy: 0.9097\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.2613 - accuracy: 0.8764 - val_loss: 0.1965 - val_accuracy: 0.9097\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.2265 - accuracy: 0.8938 - val_loss: 0.1683 - val_accuracy: 0.9290\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 149s 2s/step - loss: 0.2221 - accuracy: 0.8998 - val_loss: 0.3253 - val_accuracy: 0.8871\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.2052 - accuracy: 0.9059 - val_loss: 0.1843 - val_accuracy: 0.9226\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 150s 2s/step - loss: 0.2179 - accuracy: 0.9035 - val_loss: 0.2514 - val_accuracy: 0.9000\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 151s 2s/step - loss: 0.1870 - accuracy: 0.9192 - val_loss: 0.1444 - val_accuracy: 0.9355\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 152s 2s/step - loss: 0.1813 - accuracy: 0.9212 - val_loss: 0.1386 - val_accuracy: 0.9387\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 151s 2s/step - loss: 0.1944 - accuracy: 0.9148 - val_loss: 0.1860 - val_accuracy: 0.9194\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 151s 2s/step - loss: 0.1783 - accuracy: 0.9188 - val_loss: 0.2447 - val_accuracy: 0.9097\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 151s 2s/step - loss: 0.1653 - accuracy: 0.9253 - val_loss: 0.1075 - val_accuracy: 0.9452\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 152s 2s/step - loss: 0.1543 - accuracy: 0.9321 - val_loss: 0.2438 - val_accuracy: 0.9194\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 153s 2s/step - loss: 0.1513 - accuracy: 0.9301 - val_loss: 0.1193 - val_accuracy: 0.9419\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 152s 2s/step - loss: 0.1532 - accuracy: 0.9317 - val_loss: 0.1573 - val_accuracy: 0.9355\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 152s 2s/step - loss: 0.1487 - accuracy: 0.9362 - val_loss: 0.1253 - val_accuracy: 0.9452\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 153s 2s/step - loss: 0.1390 - accuracy: 0.9382 - val_loss: 0.1258 - val_accuracy: 0.9419\n",
      "10/10 [==============================] - 4s 417ms/step - loss: 0.1075 - accuracy: 0.9452\n",
      "Validation Loss: 0.10753267258405685\n",
      "Validation Accuracy: 0.9451612830162048\n",
      "10/10 [==============================] - 4s 402ms/step\n",
      "Confusion Matrix:\n",
      "[[  7  37]\n",
      " [ 36 230]]\n",
      "Precision: 0.8614232209737828\n",
      "Recall: 0.8646616541353384\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, evaluating, and saving a CNN model for binary classification\n",
    "\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import save_model\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "save_model(cnn_model, 'binary_cnn_model.h5')\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes)\n",
    "recall = recall_score(true_classes, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Added Layers and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.4637 - accuracy: 0.8489 - val_loss: 0.4793 - val_accuracy: 0.8581\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 351s 4s/step - loss: 0.4261 - accuracy: 0.8586 - val_loss: 0.3783 - val_accuracy: 0.8581\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 353s 5s/step - loss: 0.3817 - accuracy: 0.8586 - val_loss: 0.2737 - val_accuracy: 0.8581\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.3291 - accuracy: 0.8582 - val_loss: 0.2630 - val_accuracy: 0.8613\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.2740 - accuracy: 0.8748 - val_loss: 0.1977 - val_accuracy: 0.9161\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.2446 - accuracy: 0.8821 - val_loss: 0.2042 - val_accuracy: 0.9000\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 359s 5s/step - loss: 0.2402 - accuracy: 0.8885 - val_loss: 0.1391 - val_accuracy: 0.9419\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1987 - accuracy: 0.9095 - val_loss: 0.2492 - val_accuracy: 0.9032\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.2075 - accuracy: 0.9019 - val_loss: 0.1316 - val_accuracy: 0.9484\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1723 - accuracy: 0.9221 - val_loss: 0.3290 - val_accuracy: 0.8935\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1810 - accuracy: 0.9168 - val_loss: 0.1150 - val_accuracy: 0.9419\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 359s 5s/step - loss: 0.1567 - accuracy: 0.9285 - val_loss: 0.1302 - val_accuracy: 0.9419\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 360s 5s/step - loss: 0.1599 - accuracy: 0.9313 - val_loss: 0.1351 - val_accuracy: 0.9419\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1554 - accuracy: 0.9338 - val_loss: 0.1132 - val_accuracy: 0.9581\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1423 - accuracy: 0.9398 - val_loss: 0.1021 - val_accuracy: 0.9484\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1464 - accuracy: 0.9354 - val_loss: 0.1483 - val_accuracy: 0.9355\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1360 - accuracy: 0.9398 - val_loss: 0.1015 - val_accuracy: 0.9516\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1350 - accuracy: 0.9426 - val_loss: 0.1550 - val_accuracy: 0.9419\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.1231 - accuracy: 0.9451 - val_loss: 0.0991 - val_accuracy: 0.9516\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.1256 - accuracy: 0.9451 - val_loss: 0.1014 - val_accuracy: 0.9484\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1161 - accuracy: 0.9443 - val_loss: 0.1712 - val_accuracy: 0.9452\n",
      "Epoch 22/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.1244 - accuracy: 0.9426 - val_loss: 0.1314 - val_accuracy: 0.9355\n",
      "Epoch 23/50\n",
      "78/78 [==============================] - 359s 5s/step - loss: 0.1122 - accuracy: 0.9540 - val_loss: 0.0812 - val_accuracy: 0.9613\n",
      "Epoch 24/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.1174 - accuracy: 0.9487 - val_loss: 0.1045 - val_accuracy: 0.9484\n",
      "Epoch 25/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1036 - accuracy: 0.9564 - val_loss: 0.1109 - val_accuracy: 0.9452\n",
      "Epoch 26/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1106 - accuracy: 0.9527 - val_loss: 0.1100 - val_accuracy: 0.9516\n",
      "Epoch 27/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.1088 - accuracy: 0.9556 - val_loss: 0.3312 - val_accuracy: 0.9129\n",
      "Epoch 28/50\n",
      "78/78 [==============================] - 357s 5s/step - loss: 0.1166 - accuracy: 0.9499 - val_loss: 0.0891 - val_accuracy: 0.9613\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.0812 - accuracy: 0.9613\n",
      "Validation Loss: 0.08121751993894577\n",
      "Validation Accuracy: 0.9612902998924255\n",
      "10/10 [==============================] - 11s 1s/step\n",
      "Confusion Matrix:\n",
      "[[  3  41]\n",
      " [ 43 223]]\n",
      "Precision: 0.8446969696969697\n",
      "Recall: 0.8383458646616542\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, evaluating, and saving an enhanced CNN model for binary classification\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "save_model(cnn_model, 'enhanced_cnn_model.h5')\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes)\n",
    "recall = recall_score(true_classes, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Layers and Adjusting Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 446s 6s/step - loss: 0.4651 - accuracy: 0.8506 - val_loss: 0.4498 - val_accuracy: 0.8581\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 435s 6s/step - loss: 0.4333 - accuracy: 0.8586 - val_loss: 0.4363 - val_accuracy: 0.8581\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 360s 5s/step - loss: 0.3960 - accuracy: 0.8582 - val_loss: 0.3924 - val_accuracy: 0.8581\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 351s 5s/step - loss: 0.3482 - accuracy: 0.8599 - val_loss: 0.2986 - val_accuracy: 0.8581\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 351s 5s/step - loss: 0.3108 - accuracy: 0.8679 - val_loss: 0.2858 - val_accuracy: 0.8548\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.2855 - accuracy: 0.8679 - val_loss: 0.3042 - val_accuracy: 0.8548\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 351s 5s/step - loss: 0.2608 - accuracy: 0.8683 - val_loss: 0.1942 - val_accuracy: 0.9097\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.2505 - accuracy: 0.8833 - val_loss: 0.1831 - val_accuracy: 0.9194\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.2354 - accuracy: 0.8873 - val_loss: 0.1838 - val_accuracy: 0.9194\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 351s 5s/step - loss: 0.2332 - accuracy: 0.8926 - val_loss: 0.2737 - val_accuracy: 0.8871\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1902 - accuracy: 0.9107 - val_loss: 0.1410 - val_accuracy: 0.9387\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 354s 5s/step - loss: 0.2207 - accuracy: 0.9055 - val_loss: 0.3438 - val_accuracy: 0.8903\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 356s 5s/step - loss: 0.1887 - accuracy: 0.9152 - val_loss: 0.1468 - val_accuracy: 0.9323\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1805 - accuracy: 0.9156 - val_loss: 0.1662 - val_accuracy: 0.9194\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 353s 5s/step - loss: 0.2003 - accuracy: 0.9148 - val_loss: 0.3341 - val_accuracy: 0.8903\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 355s 5s/step - loss: 0.1882 - accuracy: 0.9160 - val_loss: 0.1543 - val_accuracy: 0.9290\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.1410 - accuracy: 0.9387\n",
      "Validation Loss: 0.14099209010601044\n",
      "Validation Accuracy: 0.9387096762657166\n",
      "10/10 [==============================] - 11s 1s/step\n",
      "Confusion Matrix:\n",
      "[[  6  38]\n",
      " [ 35 231]]\n",
      "Precision: 0.8587360594795539\n",
      "Recall: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "# Creating, compiling, training, evaluating, and saving a more complex CNN model with adjusted learning rate\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.00005),  # Reduced learning rate for finer tuning\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = cnn_model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "save_model(cnn_model, 'complex_cnn_model.h5')\n",
    "\n",
    "val_generator.reset()\n",
    "predictions = cnn_model.predict(val_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes)\n",
    "recall = recall_score(true_classes, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 13s 1s/step - loss: 0.1526 - accuracy: 0.9226\n",
      "Test Loss: 0.152609184384346\n",
      "Test Accuracy: 0.9225806593894958\n",
      "10/10 [==============================] - 13s 1s/step\n",
      "Confusion Matrix:\n",
      "[[  7  37]\n",
      " [ 37 229]]\n",
      "Precision: 0.8609022556390977\n",
      "Recall: 0.8609022556390977\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the final model on the test data\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "cnn_model = load_model('complex_cnn_model.h5')\n",
    "\n",
    "test_loss, test_accuracy = cnn_model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "test_generator.reset()\n",
    "predictions = cnn_model.predict(test_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes)\n",
    "recall = recall_score(true_classes, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
